<!doctype html><html lang=ja-jp dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="移行の実施 #  移行前の状態（Phase 1）から移行後の状態（Phase 2）までのステップは次のような経路で実施しました。
   Phase 経路     Phase 1 Apache → nginx → Container   Phase 2 Apache → istio-ingressgateway → App(Docker Swarm)   Phase 3 Apache → istio-ingressgateway → App(Kubernetes)    Apache による経路変更はパス単位（URI 単位）で実施できるため、流量が明らかに少ないパスを管理するマイクロサービスから移行を実施しました。
移行の流れ #  移行時の細かい手順は次のようになります。
 ApacheのBalancerMemberを利用してからPhase 1からPhase 2に切り替え istio-ingressgatewayとKubernetes内のネットワーク系の状態を確認  負荷試験の結果と照らし合わせて Gatewayのリソース使用量などを見る   Phase 3への切り替え前に、Virtual Serviceで特定のHeaderかQuery Parameterを利用して移行後のPodに対してアクセス。 Kubernetes内への疎通も確認できた後、istio-ingressgatewayのTraffic Weightを完全に切り替える  rps がそこまで高くない BFF はこの手順を繰り返すことで移行を淡々とすすめることができました。 高 rps の BFF サーバーはこの手順でやるにはリスクが高いので、トラフィックのミラーリングを実施して Gateway と Kubernetes クラスター全体の状態を確認していきます。 アクセスログで紹介したようにPodにログ出力のためのnginxが含まれるため、二重計上されないためにNodePortをistio-proxyに向けたものをミラーリングのためのポートとして提供しています。 nginxのミラーリングによって高rpsの時間変化がDataDogに蓄積され、そこから対応表を用いてリソースの逆算を実施し、移行フェーズへステップを進めることができました。"><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="移行の実施"><meta property="og:description" content="移行の実施 #  移行前の状態（Phase 1）から移行後の状態（Phase 2）までのステップは次のような経路で実施しました。
   Phase 経路     Phase 1 Apache → nginx → Container   Phase 2 Apache → istio-ingressgateway → App(Docker Swarm)   Phase 3 Apache → istio-ingressgateway → App(Kubernetes)    Apache による経路変更はパス単位（URI 単位）で実施できるため、流量が明らかに少ないパスを管理するマイクロサービスから移行を実施しました。
移行の流れ #  移行時の細かい手順は次のようになります。
 ApacheのBalancerMemberを利用してからPhase 1からPhase 2に切り替え istio-ingressgatewayとKubernetes内のネットワーク系の状態を確認  負荷試験の結果と照らし合わせて Gatewayのリソース使用量などを見る   Phase 3への切り替え前に、Virtual Serviceで特定のHeaderかQuery Parameterを利用して移行後のPodに対してアクセス。 Kubernetes内への疎通も確認できた後、istio-ingressgatewayのTraffic Weightを完全に切り替える  rps がそこまで高くない BFF はこの手順を繰り返すことで移行を淡々とすすめることができました。 高 rps の BFF サーバーはこの手順でやるにはリスクが高いので、トラフィックのミラーリングを実施して Gateway と Kubernetes クラスター全体の状態を確認していきます。 アクセスログで紹介したようにPodにログ出力のためのnginxが含まれるため、二重計上されないためにNodePortをistio-proxyに向けたものをミラーリングのためのポートとして提供しています。 nginxのミラーリングによって高rpsの時間変化がDataDogに蓄積され、そこから対応表を用いてリソースの逆算を実施し、移行フェーズへステップを進めることができました。"><meta property="og:type" content="article"><meta property="og:url" content="https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/migrate-practice/migrate-docker-swarm-to-kubernetes/"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2022-06-02T17:14:12+09:00"><title>移行の実施 | ニコニコ生放送 Webフロントエンド Kubernetes移行ハンドブック 2022</title><link rel=manifest href=/nicolive-kubernetes-migration-handbook-2022/manifest.json><link rel=icon href=/nicolive-kubernetes-migration-handbook-2022/favicon.png type=image/x-icon><link rel=stylesheet href=/nicolive-kubernetes-migration-handbook-2022/book.min.fcc3ce6727c2b2b91bc9a518374b77e2097222f7fe4dc0016db675da315da284.css integrity="sha256-/MPOZyfCsrkbyaUYN0t34glyIvf+TcABbbZ12jFdooQ=" crossorigin=anonymous><script defer src=/nicolive-kubernetes-migration-handbook-2022/flexsearch.min.js></script>
<script defer src=/nicolive-kubernetes-migration-handbook-2022/en.search.min.316dfec6e198e1d4e40887d7250e8b92d7bde449cce89841b69417adbdc06208.js integrity="sha256-MW3+xuGY4dTkCIfXJQ6Lkte95EnM6JhBtpQXrb3AYgg=" crossorigin=anonymous></script>
<script defer src=/nicolive-kubernetes-migration-handbook-2022/sw.min.dd2d6bf3c93d9223651db8ea5867f95fed658724b711f7b9e9e1d8aa46a84ebd.js integrity="sha256-3S1r88k9kiNlHbjqWGf5X+1lhyS3Efe56eHYqkaoTr0=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/nicolive-kubernetes-migration-handbook-2022/><span>ニコニコ生放送 Webフロントエンド Kubernetes移行ハンドブック 2022</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><span>ネットワーク</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/network/architecture/>移行前・移行中・移行後のネットワーク設計</a></li></ul></li><li><span>Manifest管理</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/manifest/manifest-management/>KubernetesのManifest管理</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/manifest/kubernetes-manifest-written-by-typescript/>TypeScriptでKubernetesのmanifestを記述する</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/manifest/kubernetes-manifest-generator-architecture/>TypeScriptでManifestを生成するGeneratorのアーキテクチャ</a></li></ul></li><li><span>Continuous Delivery</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/ci/argo-cd/>Argo CDの利用</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/ci/argo-rollouts/>Argo Rolloutsの利用</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/ci/slack-bot/>Slack Botによる自動化</a></li></ul></li><li><span>Service Mesh (Istio)</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/service-mesh/istio/>BFFとIstio</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/service-mesh/access-log/>アクセスログ</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/service-mesh/traffic-management/>Istio Ingress Gatewayの設定</a></li></ul></li><li><span>Rate Limit</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/rate-limit/global-ratelimit/>Global RateLimit</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/rate-limit/local-ratelimit/>Local RateLimit</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/rate-limit/ratelimit-is-unless/>RateLimitで負荷の上昇を防げないパターン</a></li></ul></li><li><span>スケーリング</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/scalability/horizontal-pod-autoscaler/>水平スケール</a></li></ul></li><li><span>負荷</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/performance/load-test/>負荷試験</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/performance/monitoring/>モニタリング</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/performance/load-balancing/>負荷分散</a></li></ul></li><li><span>Docker SwarmからKubernetesへの移行</span><ul><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/migrate-practice/migrate-docker-swarm-to-kubernetes/ class=active>移行の実施</a></li><li><a href=/nicolive-kubernetes-migration-handbook-2022/docs/migrate-practice/application/>アプリケーションの移行</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/nicolive-kubernetes-migration-handbook-2022/svg/menu.svg class=book-icon alt=Menu></label>
<strong>移行の実施</strong>
<label for=toc-control><img src=/nicolive-kubernetes-migration-handbook-2022/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#移行の流れ>移行の流れ</a></li><li><a href=#ロールバック設計>ロールバック設計</a></li><li><a href=#スケジュールの振り返り>スケジュールの振り返り</a></li><li><a href=#移行中移行後の運用>移行中・移行後の運用</a></li><li><a href=#まとめ>まとめ</a></li></ul></nav></aside></header><article class=markdown><h1 id=移行の実施>移行の実施
<a class=anchor href=#%e7%a7%bb%e8%a1%8c%e3%81%ae%e5%ae%9f%e6%96%bd>#</a></h1><p>移行前の状態（Phase 1）から移行後の状態（Phase 2）までのステップは次のような経路で実施しました。</p><p><img src=../migrate-network.svg alt=Kubernetesでのネットワーク></p><table><thead><tr><th style=text-align:left>Phase</th><th style=text-align:left>経路</th></tr></thead><tbody><tr><td style=text-align:left>Phase 1</td><td style=text-align:left>Apache → nginx → Container</td></tr><tr><td style=text-align:left>Phase 2</td><td style=text-align:left>Apache → istio-ingressgateway → App(Docker Swarm)</td></tr><tr><td style=text-align:left>Phase 3</td><td style=text-align:left>Apache → istio-ingressgateway → App(Kubernetes)</td></tr></tbody></table><p>Apache による経路変更はパス単位（URI 単位）で実施できるため、流量が明らかに少ないパスを管理するマイクロサービスから移行を実施しました。</p><h2 id=移行の流れ>移行の流れ
<a class=anchor href=#%e7%a7%bb%e8%a1%8c%e3%81%ae%e6%b5%81%e3%82%8c>#</a></h2><p>移行時の細かい手順は次のようになります。</p><ol><li>ApacheのBalancerMemberを利用してからPhase 1からPhase 2に切り替え</li><li>istio-ingressgatewayとKubernetes内のネットワーク系の状態を確認<ul><li>負荷試験の結果と照らし合わせて Gatewayのリソース使用量などを見る</li></ul></li><li>Phase 3への切り替え前に、Virtual Serviceで特定のHeaderかQuery Parameterを利用して移行後のPodに対してアクセス。</li><li>Kubernetes内への疎通も確認できた後、istio-ingressgatewayのTraffic Weightを完全に切り替える</li></ol><p>rps がそこまで高くない BFF はこの手順を繰り返すことで移行を淡々とすすめることができました。
高 rps の BFF サーバーはこの手順でやるにはリスクが高いので、トラフィックのミラーリングを実施して Gateway と Kubernetes クラスター全体の状態を確認していきます。
<a href=/docs/05/ingress-gateway/>アクセスログ</a>で紹介したようにPodにログ出力のための<code>nginx</code>が含まれるため、二重計上されないために<code>NodePort</code>を<code>istio-proxy</code>に向けたものをミラーリングのためのポートとして提供しています。
nginxのミラーリングによって高rpsの時間変化がDataDogに蓄積され、そこから<a href=/docs/07/horizontal-pod-autoscaler/#%e3%83%aa%e3%82%bd%e3%83%bc%e3%82%b9%e3%81%ae%e5%80%a4%e3%82%92%e3%81%a9%e3%81%86%e3%82%84%e3%81%a3%e3%81%a6%e6%b1%ba%e3%82%81%e3%82%8b%e3%81%8b>対応表</a>を用いてリソースの逆算を実施し、移行フェーズへステップを進めることができました。</p><p><a href=/docs/08/loadtest/#proxy%e3%81%8b%e3%82%89%e3%83%aa%e3%82%af%e3%82%a8%e3%82%b9%e3%83%88%e3%82%92mirroring%e3%81%99%e3%82%8b><img src=../../08/mirroring.svg alt=リクエストのミラーリング概略図></a></p><h2 id=ロールバック設計>ロールバック設計
<a class=anchor href=#%e3%83%ad%e3%83%bc%e3%83%ab%e3%83%90%e3%83%83%e3%82%af%e8%a8%ad%e8%a8%88>#</a></h2><p>Phase 1, 2, 3 で移行ステップが区切られているのはロールバックのためです。
Istio の Virtual ServiceやGatewayに指定するパラメーターはAllow List形式であるため、明示的に指定しなければ疎通が取れません（全部通す設定も可能ではある）。
ゆえにアプリケーション側で必要なURIが開放されていない場合などにエラーが発生するため、ロールバックする可能性が十分にありました。
即時性を考えた結果、Phase1と2の状態を用意することで影響範囲に応じて即ロールバックできる状態にすることで落ち着きました。</p><p>結果は何度か Phase 1、2の状態に戻すことはありました。ただ、これによって得られたものは、
Manifestにアプリケーションのルーティングの仕様が明示的に記述されるようになり、忘却されにくい状態になりました。</p><h2 id=スケジュールの振り返り>スケジュールの振り返り
<a class=anchor href=#%e3%82%b9%e3%82%b1%e3%82%b8%e3%83%a5%e3%83%bc%e3%83%ab%e3%81%ae%e6%8c%af%e3%82%8a%e8%bf%94%e3%82%8a>#</a></h2><table><thead><tr><th style=text-align:left>時期</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left>2021/07</td><td style=text-align:left>Kubernetesの移行作業着手</td></tr><tr><td style=text-align:left>2021/12</td><td style=text-align:left>Production環境でのKubernetes移行実施</td></tr><tr><td style=text-align:left>2022/03</td><td style=text-align:left>Production環境での移行作業完了</td></tr></tbody></table><p>この間、Kubernetes 自体や Argo 系のアプリケーションの更新も実施ししています。</p><table><thead><tr><th style=text-align:left>リリース時期</th><th style=text-align:left>リリースされたもの</th></tr></thead><tbody><tr><td style=text-align:left>2021/04/09</td><td style=text-align:left><a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.21.0>Kubernetes v1.21.0</a></td></tr><tr><td style=text-align:left>2021/08/05</td><td style=text-align:left><a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.22.0>Kubernetes v1.22.0</a></td></tr><tr><td style=text-align:left>2021/08/20</td><td style=text-align:left><a href=https://github.com/argoproj/argo-cd/releases/tag/v2.1.0>Argo CD v2.1.0</a></td></tr><tr><td style=text-align:left>2021/10/13</td><td style=text-align:left><a href=https://github.com/argoproj/argo-rollouts/releases/tag/v1.1.0>Argo Rollouts v1.1.0</a></td></tr><tr><td style=text-align:left>2021/12/08</td><td style=text-align:left><a href=https://github.com/kubernetes/kubernetes/releases/tag/v1.23.0>Kubernetes v1.23.0</a></td></tr><tr><td style=text-align:left>2021/12/15</td><td style=text-align:left><a href=https://github.com/argoproj/argo-cd/releases/tag/v2.2.0>Argo CD v2.2.0</a></td></tr><tr><td style=text-align:left>2022/03/06</td><td style=text-align:left><a href=https://github.com/argoproj/argo-cd/releases/tag/v2.3.0>Argo CD v2.3.0</a></td></tr><tr><td style=text-align:left>2022/03/22</td><td style=text-align:left><a href=https://github.com/argoproj/argo-rollouts/releases/tag/v1.2.0>Argo Rollouts v1.2.0</a></td></tr></tbody></table><p>※ minor バージョンは省略</p><h2 id=移行中移行後の運用>移行中・移行後の運用
<a class=anchor href=#%e7%a7%bb%e8%a1%8c%e4%b8%ad%e7%a7%bb%e8%a1%8c%e5%be%8c%e3%81%ae%e9%81%8b%e7%94%a8>#</a></h2><p>移行期間中、ArgoCDのGitOpsに則り、Manifestを管理するPull Requestを担当者が出す方式を取っていました。
しかしながら、高頻度で更新されるアプリケーションはこれが手間であるため、Slackからリリースに必要な準備一式が整うように調整しました。</p><p>Docker Swarmからのデプロイ手順はSlack上でコマンドを打つことで準備ができるようになり、
Kubernetesどころかリポジトリそのものを意識することが減りました。より詳細は<a href=/docs/04/slack-bot/>Slack Botによる自動化</a>に書いています。</p><h2 id=まとめ>まとめ
<a class=anchor href=#%e3%81%be%e3%81%a8%e3%82%81>#</a></h2><p>仕事は段取り八分とよくいったもので、移行に要した時間のほとんどが検証作業に費やされています。
Kubernetesに加え、サービスメッシュの導入によってObservabilityが向上し、リアルタイムで多くの情報が得られました。
移行期間中もリソース消費の予測がかなり簡単にでき、定量的に決定できたことは今後もデプロイを確実かつスムーズにするのに大いに役に立つと考えられます。</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(n){const e=window.getSelection(),t=document.createRange();t.selectNodeContents(n),e.removeAllRanges(),e.addRange(t)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#移行の流れ>移行の流れ</a></li><li><a href=#ロールバック設計>ロールバック設計</a></li><li><a href=#スケジュールの振り返り>スケジュールの振り返り</a></li><li><a href=#移行中移行後の運用>移行中・移行後の運用</a></li><li><a href=#まとめ>まとめ</a></li></ul></nav></div></aside></main></body></html>