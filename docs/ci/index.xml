<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Continuous Delivery on ニコニコ生放送 Webフロントエンド Kubernetes移行ハンドブック 2022</title><link>https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/</link><description>Recent content in Continuous Delivery on ニコニコ生放送 Webフロントエンド Kubernetes移行ハンドブック 2022</description><generator>Hugo -- gohugo.io</generator><language>ja-JP</language><atom:link href="https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/index.xml" rel="self" type="application/rss+xml"/><item><title>Argo CDの利用</title><link>https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/argo-cd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/argo-cd/</guid><description>Argo CD # ニコニコ生放送のフロントエンドではContinuous Delivery(以降CD)ツールとしてArgo CDとArgo Rolloutsを利用しています。 ここでの運用とその設計について紹介します。
注意書き
argoproj.io/v1alpha1/Applicationのことを「ArgoCDのApplication」と表記します。 他チームとの棲み分け # Argo CDはフロントエンドのチームだけではなく、他のチームが管理するものも存在しています。 したがってチーム横断で管理している部分が存在するとレビューコストが上がるため、App of Apps Patternsを利用して管理するArgocd Applicationをフロントエンドチームのnamespaceで分離しました。
具体的にはapp of appsを2段階で利用して次の図ように分離しています。
図中のRoot ArgoCD Appsは他チームと干渉する部分になっています。 ここに、フロントエンドチームが管理するArgoCD Appsを配置します
apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: frontend-apps finalizers: - resources-finalizer.argocd.argoproj.io spec: project: default source: targetRevision: master repoURL: # フロントエンドチームが管理するapp of appsパターンの親リポジトリ path: kubernetes/overlays/[環境名] destination: server: https://kubernetes.default.svc namespace: argocd syncPolicy: automated: {} これにより、ArgoCD上で他チームと干渉する場所が木の間にマニフェストファイルが絞り込まれました。</description></item><item><title>Argo Rolloutsの利用</title><link>https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/argo-rollouts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/argo-rollouts/</guid><description>Argo Rolloutsの導入 # Argo Rolloutsとは # Argo RolloutsはKubernetes上にPodをデプロイする方法の選択肢を増やしてくれます。 Blue/Greenデプロイ、Canaryデプロイ、Progressive Deliveryなど。
とくにTraffic Managementを利用したデプロイ方法は非常に魅力的で、利用しない理由は見当たりませんでした。 ちょうどArgo Rolloutsがv1系が利用可能な状態で、移行時の検証と同時に必要な機能が使えることを確認できたため導入しました。
2021/05 v1.0.0 2021/10 v1.1.0 2022/03 v1.2.0 Istio + Argo Rollouts # Istio自体はすでに利用可能な状態にあったため、Traffic Managementを実施するLoadBalancerはIstioを利用しています。
Istio - Traffic Management 他と比較はできていませんが、IstioでTraffic Managementをすると、IstioのService Meshの恩恵をそのまま得られることができCanaryデプロイ時にTraffic Weightが変化していることが観測できるようになります。 なお、Istio Ingress Gatewayの設定でその他の機能についても紹介しています。
Canary Deployを実施する # RolloutのManifestは.spec.strategy以外の部分はDeploymentと同じです。
apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: my-app spec: strategy: canary: trafficRouting: istio: virtualService: name: my-app routes: - http-my-app-primary maxSurge: 33% canaryService: my-app-canary stableService: my-app-stable dynamicStableScale: true steps: - setCanaryScale: matchTrafficWeight: true - pause: duration: 60 - setWeight: 34 - setCanaryScale: matchTrafficWeight: true - pause: duration: 60 - setWeight: 67 - setCanaryScale: matchTrafficWeight: true - pause: duration: 60 - setWeight: 100 特徴的なのはcanaryServiceとstableService用に2つのService定義が必要になるところです。 Rolloutsに定義されたServiceは</description></item><item><title>Slack Botによる自動化</title><link>https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/slack-bot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://dwango.github.io/nicolive-kubernetes-migration-handbook-2022/docs/ci/slack-bot/</guid><description>Slack Botによる自動化 # Argo CDによるGitOpsの実現は同時にGitOpsを開発者に強制します。 すなわち、バージョンアップのためのcommitを実施し、Pull Requestを投げ、マージする必要があります。 更新頻度の多いアプリケーションを抱えた場合、この作業が非常に長く開発者の体験を悪くします。
そこで、Slack Botサーバーを作成しSlackにメッセージを入力することで手続き的なタスクをサーバー側に実施するようにしました。
バージョンアップのシーケンス図 # シーケンス図を使って紹介します。 バージョンアップの手順はSlack上でBotに対して次のようなコマンドを投げることから始まります。
# server-aをバージョン2.0.0に変更する @bot update version:2.0.0 app:servive-a これを受け取ったbotサーバーは、メッセージの入力者を判別したり、コマンド(update version)をパースしたりします。コマンドに応じてGitHub APIをCallし、JSONで記述されたファイル(User Config)を書き換え、commitします。 その後Pull Requestを作成して、結果をユーザーに返します。
作成されたPull RequestをさらにSlackからマージします。
@bot merge pr:123 これをシーケンス図で書き起こすと次のようになります。
基本的な操作はすべてSlack上から実施が可能で、開発者がバージョンアップのためにリポジトリをCloneして環境構築する必要はありません。
既存のアプリケーションのCIとKubernetes Manifestのリポジトリの連携 # Slack Botによって自動化されたKubernetesのManifestリポジトリは既存のリリースフローとも結合が容易になります。
例えば、アプリケーションにバージョンアップのCIタスクがあった場合、次のバージョン情報をSlackのWebhookを利用して先程と同じようにメッセージをBotに対して送るだけで結合できます。
# 擬似コード message=&amp;#34;{\&amp;#34;text\&amp;#34;:\&amp;#34;@bot update version:${nextVersion}app:service-a\&amp;#34;}&amp;#34; curl -X POST -H &amp;#39;Content-type: application/json&amp;#39; --data $message https://hooks.slack.com/services/{your_id} 大抵のサーバーはcurlかそれに類するHTTP Clientを用意できるため、たった2行挿入するだけでデプロイの簡略化ができます。
Slack Botによってデプロイ作業を最小工数で終わらせる # バージョンアップのコマンドを紹介しましたが、他にも10個程度のコマンドがあります。
リリース準備用のコマンド 最新のリリース情報の取得 次に投入される予定のバージョン情報の取得（差分） リリース用のチケット作成 リリースノート更新 など、リリースに関する一連の情報や作業が細かくできるようになっています。 特に、差分情報やリリースノートの作成などを自動で実施しているためリリースの影響範囲が単純明快になるため確認コストが最小限になっています。</description></item></channel></rss>